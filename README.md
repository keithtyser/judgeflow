
# JudgeFlowÂ ğŸš¦ â€“ Modular, Responsible LLM Evaluation Framework
> Plugâ€‘in new metrics by dropping a YAML, run one command, get a CSVâ€‘gradeâ€‘sheet & reliability diagram.

---

## âœ¨Â Why JudgeFlow?

| Feature | JudgeFlow | Existing toolkits* |
|---------|-----------|--------------------|
| **Metric as YAML** | **Yes â€“ drop a file, no code** | Hard-coded in Python |
| **Responsible-AI coverage** | Truthfulness, toxicity, fairness, privacy, calibration | Mostly quality metrics |
| **Score reliability** | Self-reflection + agreement checks | Rarely included |
| **Confidence calibration** | Built-in calibration curve | Not available |
| **Interoperability** | Pre-trained, RLHF, agent traces | Single prompt/response |
| **Outputs** | CLI + CSV (MLOps-friendly) | Custom dashboards |

<sup>DeepEval remains an excellent coherence scorer â€“ and JudgeFlow can call it when you pass `--deepeval`.</sup>

---

## ğŸš€Â Quickstart

```bash
pip install -r requirements.txt                       # 1. deps
python download_mini_datasets.py                      # 2. test on 60â€‘row toy data
export OPENAI_API_KEY=skâ€‘...                          # 3. key
python -m src.judgeflow.cli -d datasets/mmlu.parquet  # 4. go!
open scores.csv                                       # 5. inspect
```

Want a oneâ€‘liner reliability diagram?

```bash
python plot_calibration_curve.py --csv scores.csv
```

---

## ğŸ—ï¸Â Architecture at a Glance
```
parquet â†’ Runner â†’ [async metric coroutines] â†’ CSV
                         â†‘            â”‚
              MetricSpec (YAML)       â””â”€â†’ optional selfâ€‘reflection / resamples
```

* **LLMAdapter** â€“ pluggable async wrapper (GPTâ€‘4 by default, swap to Anthropic, vLLM, or any OpenAIâ€‘compatible endpoint).  
* **MetricSpec** â€“ Pydanticâ€‘typed YAML: prompt template, regex parser, selfâ€‘reflection & confidence prompts.  
* **Runner** â€“ reads dataset, spawns perâ€‘row tasks, writes tidy CSV.  
* **rai_helpers** â€“ outâ€‘ofâ€‘theâ€‘box fairness, toxicity, PII, calibration utilities.



---

## ğŸ”ŒÂ Defining Your Own Metric (30â€¯s)

```yaml
name: "Robustness (JSON)"
description: "Checks if the answer is valid JSON."
prompt_template: |
  Answer:
  {answer}

  Reply 0â€‘10: how wellâ€‘formed & informative is the JSON?
parser: 'regex:(\d+(?:\.\d+)?)'
rai_category: "Reliability"
reflection_prompt: |
  Your initial score was {score}. Give a revised score as 'Revised score: X' plus oneâ€‘sentence critique.
confidence_prompt: |
  How confident (0â€‘100) are you in that score?
```

Save as `src/judgeflow/metrics/robust_json.yaml` and reâ€‘run the CLI â€“ nothing else to code.

---

## ğŸ“ŠÂ Firstâ€‘Class Responsibleâ€‘AI Metrics

| Category | Metric | Implementation |
|----------|--------|----------------|
| **Truthfulness** | Factuality | LLM rubric |
| **Quality** | Coherence, Reasoning | LLM rubric, optional DeepEval |
| **Safety** | Toxicity | Detoxify |
| **Privacy** | PII leak | spaCy + regex |
| **Fairness** | Statistical Parity &Â TPR Gap<br>Demographicâ€¯Parity, Equalâ€¯Opportunity, Calibrationâ€¯Gap | Handâ€‘rolled + Fairlearn |
| **Reliability** | Confidence Calibration Curve | sklearn.calibration |

Each YAML can inject preâ€‘computed numbers (`dp_diff`, `calib_gap`, â€¦) produced by helper functions.

---

## ğŸ”§Â UseÂ Cases

| Scenario | How JudgeFlow Helps |
|----------|--------------------|
| **Redâ€‘team a new chat assistant** | Combine toxicity, PII leak, and fairness metrics; triage by low selfâ€‘confidence scores. |
| **Compare RLHF vs. base model** | Point Runner at two CSVs, analyze score deltas & calibration. |
| **Audit an autonomous agent chain** | Treat the full treeâ€‘ofâ€‘thought trace as `answer`; add a metric that penalizes insecure tool calls. |
| **CI/CD guardâ€‘rail** | Fail the build if mean factualityÂ <â€¯7 or calibration gapÂ >â€¯0.05. |

---

## ğŸ›£ï¸Â Roadmap â€“ What Iâ€™d Build Next

* **Multiâ€‘turn & Treeâ€‘ofâ€‘Thought Support** â€“ extend Runner to accept a list of messages or JSON trace per row; write sequenceâ€‘aware metrics.  
* **Languageâ€‘agnostic Safety** â€“ plug in multilingual Detoxicity models & spaCy pipelines.  
* **Crossâ€‘model Judging** â€“ simple flag to judge outputs with a *different* model family (e.g., GPTâ€‘4 evals Llamaâ€‘2).  
* **Adapter Zoo** â€“ vLLM, Anthropic, AzureÂ OpenAI, Ollama â€“ dropâ€‘in `--backend` switch.  
* **Batch Optimization** â€“ smart prompt packing & caching to slash eval cost.  
* **Rich Reports** â€“ autoâ€‘generate HTML dashboards & perâ€‘metric leaderboards.

I spun this prototype up in a day â€“ imagine the velocity with a fullâ€‘time seat ğŸš€.

---

## ğŸ“Â Repo Tour

```
â”œâ”€â”€ src/judgeflow/
â”‚   â”œâ”€â”€ runner.py          # async orchestration
â”‚   â”œâ”€â”€ llm.py             # pluggable LLM client
â”‚   â”œâ”€â”€ metrics.py         # YAML registry loader
â”‚   â”œâ”€â”€ rai_helpers.py     # fairness / safety utils
â”‚   â””â”€â”€ metrics/*.yaml     # plugâ€‘andâ€‘play metric specs
â”œâ”€â”€ scores.csv             # sample output
â””â”€â”€ plot_calibration_curve.py
```

---

## ğŸ”–Â License
MIT â€“ free for personal & commercial use.
